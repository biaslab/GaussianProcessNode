{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/PhD/GaussianProcessNode`\n"
     ]
    }
   ],
   "source": [
    "using Pkg \n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling ReactiveMP [a194aa59-28ba-4574-a09c-4a745416d6e3]\n",
      "WARNING: using StatsFuns.logmvbeta in module ReactiveMP conflicts with an existing identifier.\n",
      "WARNING: import of Base.minimum into ReactiveMP conflicts with an existing identifier; ignored.\n",
      "WARNING: Method definition entropy(Any) in module StatsBase at /Users/nguyenhuuminhhoang/.julia/packages/StatsBase/XgjIN/src/scalarstats.jl:755 overwritten in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/nodes/gp.jl:53.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition entropy(Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/nodes/gp.jl:53 overwritten at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/nodes/tp.jl:49.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition make_productdist_message(Any, Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/rules/multiplication/out.jl:92 overwritten at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/rules/multiplication/A.jl:76.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition rule(typeof(Base.:(*)), Base.Val{:A}, ReactiveMP.Marginalisation, Base.Val{(:out, :in)}, Tuple{ReactiveMP.Message{var\"#s1287\", A} where A where var\"#s1287\"<:(Union{ReactiveMP.NormalMeanPrecision{T}, ReactiveMP.NormalMeanVariance{T}, ReactiveMP.NormalWeightedMeanPrecision{T}} where T), ReactiveMP.Message{var\"#s1165\", A} where A where var\"#s1165\"<:(Distributions.LogNormal{T} where T<:Real)}, Nothing, Nothing, ReactiveMP.ProcessMeta, Any, Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/rule.jl:272 overwritten on the same line (check for duplicate calls to `include`).\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition circularize(Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/gpstrategy.jl:536 overwritten at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/rules/gp/params.jl:359.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition fastinvmahalanobis(Any, Any, Any, Any, Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/gpstrategy.jl:587 overwritten at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/rules/gp/params.jl:371.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition returnFFTstructures(Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/gpstrategy.jl:549 overwritten at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/rules/gp/params.jl:377.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling RxInfer [86711068-29c9-4ff7-b620-ae75d7495b3d]\n",
      "WARNING: Method definition entropy(Any) in module StatsBase at /Users/nguyenhuuminhhoang/.julia/packages/StatsBase/XgjIN/src/scalarstats.jl:755 overwritten in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/nodes/tp.jl:49.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition entropy(Any) in module ReactiveMP at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/nodes/tp.jl:49 overwritten at /Users/nguyenhuuminhhoang/PhD/ReactiveMP.jl/src/nodes/gp.jl:53.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using Revise \n",
    "using StableRNGs, GraphPPL,ReactiveMP, RxInfer, Random, Distributions, LinearAlgebra, Plots\n",
    "using Flux, Zygote, ForwardDiff, Optim\n",
    "using SpecialFunctions\n",
    "using BenchmarkTools\n",
    "using  MAT, DSP, FFTW, WAV\n",
    "import KernelFunctions: SqExponentialKernel, Matern52Kernel, with_lengthscale, Kernel, kernelmatrix  \n",
    "import ReactiveMP: GaussHermiteCubature, approximate_meancov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_cleandata (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_cleandata(n, f_gp, c_gp)\n",
    "    y=Float64[]\n",
    "    obs = []\n",
    "    for i=1:n\n",
    "        temp = c_gp[i] * exp(f_gp[i]) \n",
    "        push!(y,temp)\n",
    "    end\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500; #data length\n",
    "n = 500; #lenght of the axis of gp \n",
    "θcarrier   = 0.01\n",
    "θamplitude = 1.\n",
    "mean_amplitude = (x) -> 0;\n",
    "mean_carrier   = (x) -> 0.;\n",
    "kernel_carrier(θ)     =  0.5with_lengthscale(Matern52Kernel(),θ)\n",
    "kernel_amplitude(θ)   =  with_lengthscale(Matern52Kernel(),θ) #+ with_lengthscale(PeriodicKernel(),3.0)\n",
    "tmin,tmax = 0., 5.0\n",
    "time_range = collect(range(tmin, tmax; length=n));\n",
    "Cov_amplitude = kernelmatrix(kernel_amplitude(θamplitude),time_range,time_range) + 1e-7*I;\n",
    "Cov_carrier   = kernelmatrix(kernel_carrier(θcarrier),time_range,time_range) + 1e-7I;\n",
    "gp_amplitude  = MvNormal(mean_amplitude.(time_range), Cov_amplitude)\n",
    "gp_carrier    = MvNormal(mean_carrier.(time_range), Cov_carrier)\n",
    "amplitude_gt = rand(StableRNG(1133),gp_amplitude)\n",
    "carrier_gt   = rand(StableRNG(11), gp_carrier)\n",
    "\n",
    "\n",
    "#Generate data \n",
    "y_data_synthetic = generate_cleandata(n,amplitude_gt,carrier_gt);\n",
    "slicedim(dim) = (a) -> map(e -> e[dim], a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponentiate function \n",
    "struct MyExponential end\n",
    "\n",
    "@node MyExponential Deterministic [ y, x ]   ## x: input,  y: output\n",
    "\n",
    "\n",
    "@rule MyExponential(:y, Marginalisation) (m_x::UnivariateGaussianDistributionsFamily,) = begin \n",
    "    @logscale 0\n",
    "    return LogNormal(mean(m_x), var(m_x))\n",
    "end\n",
    "\n",
    "@rule MyExponential(:x, Marginalisation) (m_y::ContinuousUnivariateLogPdf, m_x::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    dist = m_x    \n",
    "    m_in, var_in = mean_var(m_x)\n",
    "    l_pdf = (x) -> logpdf(m_y,exp.(x)) \n",
    "    pdf = x -> exp(l_pdf(x)-logpdf(dist,x)+1e-7)\n",
    "    m,v = approximate_meancov(ghcubature(131),pdf,dist)\n",
    "\n",
    "    scalefactor = (x) -> exp(l_pdf(x))/exp(-x^2)\n",
    "    x, w = ReactiveMP.gausshermite(9)\n",
    "    Z = dot(w,scalefactor.(x))\n",
    "    @logscale 1\n",
    "\n",
    "    if isnan(v) || isnan(m)\n",
    "        log_pdf = x -> l_pdf(x) + logpdf(dist,x[1])  + 1e-7\n",
    "        res = optimize(x -> -log_pdf(x), [m_in])\n",
    "        mproxy = res.minimizer[1]\n",
    "        dx  = x -> ForwardDiff.derivative(y -> -log_pdf(y),x)\n",
    "        ddx = x -> ForwardDiff.derivative(dx, x)\n",
    "\n",
    "        vproxy = cholinv(ddx(mproxy+tiny))\n",
    "        m_ = mproxy \n",
    "        v_ = vproxy + 1e-6\n",
    "\n",
    "        ksi = m_/v_ - m_in/var_in\n",
    "        precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "        if isnan(ksi) || isnan(precision)\n",
    "            samples = rand(dist,3000)\n",
    "            weights = exp.(l_pdf.(samples)) / sum(exp.(l_pdf.(samples)) )\n",
    "            if any(isnan.(weights)) \n",
    "                m_ = sum(samples)/3000\n",
    "                v_ = sum((samples .- m_).^2) /3000 \n",
    "            else\n",
    "                m_ = sum(weights .* samples)\n",
    "                v_ = sum(weights .* (samples .- m_).^2)    \n",
    "            end\n",
    "            ksi = m_/v_ - m_in/var_in\n",
    "            precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "            \n",
    "            return NormalWeightedMeanPrecision(ksi,precision)\n",
    "        else\n",
    "            return  NormalWeightedMeanPrecision(ksi,precision)\n",
    "        end\n",
    "    else\n",
    "        return  NormalMeanVariance(m,v+1e-6)\n",
    "    end\n",
    "end\n",
    "\n",
    "@marginalrule MyExponential(:x) (m_y::ContinuousUnivariateLogPdf, m_x::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    b_x = @call_rule MyExponential(:x, Marginalisation) (m_y = m_y, m_x=m_x)\n",
    "    q_x = ReactiveMP.prod(ProdAnalytical(),b_x,m_x)\n",
    "    if isinf(entropy(m_x)) || isnan(entropy(m_x))\n",
    "        q_x= m_x \n",
    "    end\n",
    "    return (x=m_x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rule typeof(*)(:A, Marginalisation) (m_out::PointMass, m_A::UnivariateGaussianDistributionsFamily, m_in::LogNormal, meta::TinyCorrection) = begin \n",
    "    @logscale 0\n",
    "    backward_A = (x) -> -log(abs(x)) + logpdf(m_in,mean(m_out)/x)\n",
    "    mean_in, var_in = mean_var(m_A)\n",
    "    dist = m_A \n",
    "    pdf = x -> exp(backward_A(x)-logpdf(dist,x)+1e-7)\n",
    "    m,v = approximate_meancov(ghcubature(121),pdf,dist)\n",
    "\n",
    "    if isnan(v) || isnan(m)\n",
    "        log_pdf = x -> backward_A(x[1]) + logpdf(dist,x[1])  + 1e-7\n",
    "        res = optimize(x -> -log_pdf(x), [mean_in])\n",
    "        mproxy = res.minimizer[1]\n",
    "        dx  = x -> ForwardDiff.derivative(y -> -log_pdf(y),x)\n",
    "        ddx = x -> ForwardDiff.derivative(dx, x)\n",
    "\n",
    "        vproxy = cholinv(ddx(mproxy+tiny))\n",
    "        m_ = mproxy \n",
    "        v_ = vproxy + 1e-6\n",
    "\n",
    "        ksi = m_/v_ - mean_in/var_in\n",
    "        precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "        if isnan(ksi) || isnan(precision)\n",
    "            samples = rand(dist,3000)\n",
    "            weights = exp.(backward_A.(samples)) / sum(exp.(backward_A.(samples)) )\n",
    "            if any(isnan.(weights)) \n",
    "                m_ = sum(samples)/3000\n",
    "                v_ = sum((samples .- m_).^2) /3000\n",
    "            else\n",
    "                m_ = sum(weights .* samples)\n",
    "                v_ = sum(weights .* (samples .- m_).^2)    \n",
    "            end\n",
    "            ksi = m_/v_ - mean_in/var_in\n",
    "            precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "            \n",
    "            return NormalWeightedMeanPrecision(ksi,precision)\n",
    "        else\n",
    "            return  NormalWeightedMeanPrecision(ksi,precision)\n",
    "        end\n",
    "    else\n",
    "        return  NormalMeanVariance(m,v+1e-6)\n",
    "    end\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:A, Marginalisation) (m_out::PointMass, m_in::UnivariateGaussianDistributionsFamily, meta::TinyCorrection) = begin \n",
    "    @logscale 0 #wrong \n",
    "    backward_A = (x) -> -log(abs(x)) + logpdf(m_in,mean(m_out)/x)\n",
    "    return ContinuousUnivariateLogPdf(backward_A)\n",
    "end\n",
    "\n",
    "@marginalrule typeof(*)(:A_in) (m_out::PointMass, m_A::NormalMeanVariance, m_in::LogNormal, meta::TinyCorrection) = begin \n",
    "    b_A = @call_rule typeof(*)(:A, Marginalisation) (m_out = m_out, m_A = m_A, m_in=m_in, meta=meta)\n",
    "    q_A = ReactiveMP.prod(ProdAnalytical(), b_A, m_A)\n",
    "    if isinf(entropy(q_A))\n",
    "        q_A = m_A\n",
    "    end\n",
    "    return (A = q_A,in = m_in, out=m_out)\n",
    "end\n",
    "\n",
    "# @rule typeof(*)(:A, Marginalisation) (m_out::NormalMeanVariance, m_A::NormalMeanPrecision, m_in::LogNormal, meta::TinyCorrection) = begin \n",
    "#     @logscale 0\n",
    "#         return  NormalMeanVariance(mean(m_out) / mean(m_in),1.)\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ReactiveMP.prod(::ProdAnalytical, left::LogNormal, right::ContinuousUnivariateLogPdf) \n",
    "    return left\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function MixturePAD(n,nmixtures, P_amp,A_amp,Q_amp,P_carr,A_carr,Q_carr,H)\n",
    "    y = datavar(Float64,n)\n",
    "\n",
    "    yk = randomvar(nmixtures,n)\n",
    "    fk_0 = randomvar(nmixtures)\n",
    "    ck_0 = randomvar(nmixtures)\n",
    "    fk = randomvar(nmixtures,n)\n",
    "    ck = randomvar(nmixtures,n)\n",
    "\n",
    "    zk = randomvar(nmixtures,n)\n",
    "    gk = randomvar(nmixtures,n)\n",
    "    dk = randomvar(nmixtures,n)\n",
    "\n",
    "    s = randomvar(n)\n",
    "\n",
    "\n",
    "    for k=1:nmixtures\n",
    "        fk_0[k] ~ MvNormalMeanCovariance(zeros(length(H)), P_amp[k])\n",
    "        ck_0[k] ~ MvNormalMeanCovariance(zeros(length(H)), P_carr[k])\n",
    "        f_prev = fk_0[k]\n",
    "        c_prev = ck_0[k]\n",
    "        for j=1:n\n",
    "            fk[k,j] ~ MvNormalMeanCovariance(A_amp[k][j] * f_prev, Q_amp[k][j])\n",
    "            zk[k,j] ~ NormalMeanVariance(dot(H , fk[k,j]), .1)\n",
    "            gk[k,j] ~ MyExponential(zk[k,j]) where {pipeline = RequireMessage(x = NormalMeanPrecision(0., 1.))}\n",
    "\n",
    "            ck[k,j] ~ MvNormalMeanCovariance(A_carr[k][j] * c_prev, Q_carr[k][j])\n",
    "            dk[k,j] ~ NormalMeanVariance(dot(H,ck[k,j]), .1) \n",
    "\n",
    "            yk[k,j] ~ (*)(dk[k,j],gk[k,j]) where {meta = TinyCorrection(), pipeline = RequireMessage(A = NormalMeanPrecision(0., 1.))}\n",
    "            f_prev = fk[k,j]\n",
    "            c_prev = ck[k,j]\n",
    "        end \n",
    "    end\n",
    "    \n",
    "    y_tup = tuple(yk...)\n",
    "    π ~ Dirichlet(ones(nmixtures))\n",
    "\n",
    "    # temp = randomvar(n)\n",
    "    for i=1:n \n",
    "        s[i] ~ Categorical(π) where { pipeline = EnforceMarginalFunctionalDependency(:out) }\n",
    "        # temp[i] ~ Mixture(s[i], y_tup[(i-1)*nmixtures+1:i*nmixtures]) where { pipeline = EnforceMarginalFunctionalDependency(:switch) }\n",
    "        # y[i] ~ NormalMeanVariance(temp[i],0.1)\n",
    "        y[i] ~ Mixture(s[i], y_tup[(i-1)*nmixtures+1:i*nmixtures]) where { pipeline = EnforceMarginalFunctionalDependency(:switch) }\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_Q (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### compute matrices for the state-space model corresponding to Matern-52 kernel ######\n",
    "\n",
    "Δt = [time_range[2] - time_range[1]]; # time difference\n",
    "append!(Δt, time_range[2:end] - time_range[1:end-1]);\n",
    "\n",
    "L = [0., 0., 1.];\n",
    "H = [1., 0., 0.];\n",
    "function compute_F(θ)\n",
    "    λ = sqrt(5)/θ[1]\n",
    "    return [0. 1. 0.; 0. 0. 1.;-λ^3 -3λ^2 -3λ]\n",
    "end\n",
    "function compute_P(θ)\n",
    "    Qc = 16/3 * θ[2] * (sqrt(5)/θ[1])^5;\n",
    "    F = compute_F(θ)\n",
    "    Imat = diageye(3)\n",
    "    P_vec = inv(kron(Imat,F) + kron(F,Imat)) * vec(-L * Qc * L')\n",
    "    return reshape(P_vec,3,3);\n",
    "end\n",
    "function compute_A(θ)\n",
    "    F = compute_F(θ)\n",
    "    A = [exp(F * i) for i in Δt];\n",
    "    return A #vector of matrix A \n",
    "end\n",
    "\n",
    "function compute_Q(θ)\n",
    "    A = compute_A(θ)\n",
    "    P = compute_P(θ)\n",
    "\n",
    "    Q = [P - i*P*i' for i in A]\n",
    "    return Q #vector of matrix Q \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmixtures = 3\n",
    "θ_amplitude = [[1., 1.], [0.5, 0.5],[0.1,0.1]\n",
    "    ]; # store [l, σ²]\n",
    "\n",
    "θ_carrier = [[0.01, .5], [0.1,0.2],[0.001,0.4]\n",
    "]; # store [l, σ²]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Matrix{Float64}}:\n",
       " [0.4999999999999979 8.253050944653583e-14 -8333.333333333336; 5.226915617886441e-14 8333.333333333336 -1.3170641551236749e-9; -8333.333333333332 1.3170641551236749e-9 1.25e9]\n",
       " [0.19999999999999968 5.784427732950662e-15 -33.33333333333331; 3.967817264071133e-15 33.33333333333331 -3.4435919832641344e-13; -33.333333333333336 3.4435919832641344e-13 50000.000000000015]\n",
       " [0.4000000000000008 -1.077101388884828e-12 -666666.6666666676; 3.520406567454182e-13 666666.6666666676 -2.4426751179531065e-7; -666666.6666666683 2.4426751179531065e-7 1.0000000000000004e13]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_amplitude_tensor = [compute_A(i) for i in θ_amplitude]\n",
    "Q_amplitude_tensor = [compute_Q(i) for i in θ_amplitude]\n",
    "P_amplitude_tensor = [compute_P(i) for i in θ_amplitude]\n",
    "\n",
    "A_carrier_tensor = [compute_A(i) for i in θ_carrier]\n",
    "Q_carrier_tensor = [compute_Q(i) for i in θ_carrier]\n",
    "P_carrier_tensor = [compute_P(i) for i in θ_carrier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rule Categorical(:p, Marginalisation) (m_out::Categorical, q_out::PointMass) = begin\n",
    "    @logscale -SpecialFunctions.logfactorial(length(probvec(q_out)))\n",
    "    return Dirichlet(probvec(q_out) .+ one(eltype(probvec(q_out))))\n",
    "end\n",
    "\n",
    "struct EnforceMarginalFunctionalDependency <: ReactiveMP.AbstractNodeFunctionalDependenciesPipeline\n",
    "    edge :: Symbol\n",
    "end\n",
    "\n",
    "function ReactiveMP.message_dependencies(::EnforceMarginalFunctionalDependency, nodeinterfaces, nodelocalmarginals, varcluster, cindex, iindex)\n",
    "    return ReactiveMP.message_dependencies(ReactiveMP.DefaultFunctionalDependencies(), nodeinterfaces, nodelocalmarginals, varcluster, cindex, iindex)\n",
    "end\n",
    "\n",
    "function ReactiveMP.marginal_dependencies(enforce::EnforceMarginalFunctionalDependency, nodeinterfaces, nodelocalmarginals, varcluster, cindex, iindex)\n",
    "    default = ReactiveMP.marginal_dependencies(ReactiveMP.DefaultFunctionalDependencies(), nodeinterfaces, nodelocalmarginals, varcluster, cindex, iindex)\n",
    "    index   = ReactiveMP.findnext(i -> name(i) === enforce.edge, nodeinterfaces, 1)\n",
    "    if index === iindex \n",
    "        return default\n",
    "    end\n",
    "    vmarginal = ReactiveMP.getmarginal(ReactiveMP.connectedvar(nodeinterfaces[index]), IncludeAll())\n",
    "    loc = ReactiveMP.FactorNodeLocalMarginal(-1, index, enforce.edge)\n",
    "    ReactiveMP.setstream!(loc, vmarginal)\n",
    "    # Find insertion position (probably might be implemented more efficiently)\n",
    "    insertafter = sum(first(el) < iindex ? 1 : 0 for el in default; init = 0)\n",
    "    return ReactiveMP.TupleTools.insertafter(default, insertafter, (loc, ))\n",
    "end\n",
    "\n",
    "# function for using hard switching\n",
    "function ReactiveMP.functional_dependencies(::EnforceMarginalFunctionalDependency, factornode::MixtureNode{N, F}, iindex::Int) where {N, F <: FullFactorisation}\n",
    "    message_dependencies = if iindex === 1\n",
    "        # output depends on:\n",
    "        (factornode.inputs,)\n",
    "    elseif iindex === 2\n",
    "        # switch depends on:\n",
    "        (factornode.out, factornode.inputs)\n",
    "    elseif 2 < iindex <= N + 2\n",
    "        # k'th input depends on:\n",
    "        (factornode.out, )\n",
    "    else\n",
    "        error(\"Bad index in functional_dependencies for SwitchNode\")\n",
    "    end\n",
    "\n",
    "    marginal_dependencies = if iindex === 1\n",
    "        # output depends on:\n",
    "        (factornode.switch,)\n",
    "    elseif iindex == 2\n",
    "        #  switch depends on\n",
    "        ()\n",
    "    elseif 2 < iindex <= N + 2\n",
    "        # k'th input depends on:\n",
    "        (factornode.switch,)\n",
    "    else\n",
    "        error(\"Bad index in function_dependencies for SwitchNode\")\n",
    "    end\n",
    "    # println(marginal_dependencies)\n",
    "    return message_dependencies, marginal_dependencies\n",
    "end\n",
    "\n",
    "# create an observable that is used to compute the switch with pipeline constraints\n",
    "function ReactiveMP.get_messages_observable(factornode::MixtureNode{N, F, Nothing, ReactiveMP.FactorNodePipeline{P, EmptyPipelineStage}}, messages::Tuple{ReactiveMP.NodeInterface, NTuple{N, ReactiveMP.IndexedNodeInterface}}) where {N, F <: FullFactorisation, P <: EnforceMarginalFunctionalDependency}\n",
    "    switchinterface  = messages[1]\n",
    "    inputsinterfaces = messages[2]\n",
    "\n",
    "    msgs_names = Val{(name(switchinterface), name(inputsinterfaces[1]))}()\n",
    "    msgs_observable =\n",
    "    combineLatest((ReactiveMP.messagein(switchinterface), combineLatest(map((input) -> ReactiveMP.messagein(input), inputsinterfaces), PushNew())), PushNew()) |>\n",
    "        map_to((ReactiveMP.messagein(switchinterface), ReactiveMP.ManyOf(map((input) -> ReactiveMP.messagein(input), inputsinterfaces))))\n",
    "    return msgs_names, msgs_observable\n",
    "end\n",
    "\n",
    "# create an observable that is used to compute the output with pipeline constraints\n",
    "function ReactiveMP.get_messages_observable(factornode::MixtureNode{N, F, Nothing, ReactiveMP.FactorNodePipeline{P, EmptyPipelineStage}}, messages::Tuple{NTuple{N, ReactiveMP.IndexedNodeInterface}}) where {N, F <: FullFactorisation, P <: EnforceMarginalFunctionalDependency}\n",
    "    inputsinterfaces = messages[1]\n",
    "\n",
    "    msgs_names = Val{(name(inputsinterfaces[1]), )}()\n",
    "    msgs_observable =\n",
    "    combineLatest(map((input) -> ReactiveMP.messagein(input), inputsinterfaces), PushNew()) |>\n",
    "        map_to((ReactiveMP.ManyOf(map((input) -> ReactiveMP.messagein(input), inputsinterfaces)),))\n",
    "    return msgs_names, msgs_observable\n",
    "end\n",
    "\n",
    "# create an observable that is used to compute the input with pipeline constraints\n",
    "function ReactiveMP.get_messages_observable(factornode::MixtureNode{N, F, Nothing, ReactiveMP.FactorNodePipeline{P, EmptyPipelineStage}}, messages::Tuple{ReactiveMP.NodeInterface}) where {N, F <: FullFactorisation, P <: EnforceMarginalFunctionalDependency}\n",
    "    outputinterface = messages[1]\n",
    "\n",
    "    msgs_names = Val{(name(outputinterface), )}()\n",
    "    msgs_observable = combineLatestUpdates((ReactiveMP.messagein(outputinterface), ), PushNew())\n",
    "    return msgs_names, msgs_observable\n",
    "end\n",
    "\n",
    "function ReactiveMP.get_marginals_observable(factornode::MixtureNode{N, F, Nothing, ReactiveMP.FactorNodePipeline{P, EmptyPipelineStage}}, marginals::Tuple{ReactiveMP.NodeInterface}) where {N, F <: FullFactorisation, P <: EnforceMarginalFunctionalDependency}\n",
    "    switchinterface = marginals[1]\n",
    "\n",
    "    marginal_names       = Val{(name(switchinterface), )}()\n",
    "    marginals_observable = combineLatestUpdates((getmarginal(ReactiveMP.connectedvar(switchinterface), IncludeAll()), ), PushNew())\n",
    "\n",
    "    return marginal_names, marginals_observable\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@constraints function constraints_MixturePAD()\n",
    "    q(s) :: PointMass\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ReactiveMP.prod(::ProdAnalytical, left::PointMass, right::UnivariateGaussianDistributionsFamily) \n",
    "    mean_left = mean(left)\n",
    "    return pdf(right,mean_left)\n",
    "end\n",
    "\n",
    "function ReactiveMP.compute_logscale(new_dist::PointMass, left_dist::PointMass, right_dist::UnivariateGaussianDistributionsFamily)\n",
    "    return 0 #wrong\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rule Mixture(:switch, Marginalisation) (m_out::PointMass, m_inputs::ManyOf{N, Any}) where {N} = begin\n",
    "\n",
    "    #  compute logscales of different products\n",
    "    # `messages` are available from the `@rule` macro itself\n",
    "    logscales = map(input -> getlogscale(input), messages[2])\n",
    "\n",
    "    @logscale logsumexp(logscales)\n",
    "\n",
    "    return Categorical(softmax(collect(logscales)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference results:\n",
       "  Posteriors       | available for (fk)\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niters = 1\n",
    "\n",
    "result = inference(\n",
    "    model = MixturePAD(N,nmixtures,P_amplitude_tensor,A_amplitude_tensor,Q_amplitude_tensor,P_carrier_tensor,A_carrier_tensor,Q_carrier_tensor,H),\n",
    "    iterations = niters,\n",
    "    data = (y=y_data_synthetic,),\n",
    "    initmessages = (π = Dirichlet(ones(nmixtures)),),\n",
    "    initmarginals = (π = Dirichlet(ones(nmixtures)),),\n",
    "    returnvars = (fk = KeepLast(),),\n",
    "    constraints = constraints_MixturePAD(),\n",
    "    addons = AddonLogScale(),\n",
    "    options = (limit_stack_depth=100,),\n",
    "    showprogress = true,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
