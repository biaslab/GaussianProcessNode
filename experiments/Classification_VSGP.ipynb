{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/PhD/GaussianProcessNode/OldEnvironment`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../OldEnvironment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ApproximateGPs\n",
    "using ParameterHandling\n",
    "using Zygote, StatsFuns\n",
    "using PDMats: PDMat\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Optim, Flux\n",
    "using IterTools: ncycle\n",
    "using Plots\n",
    "using JLD, MAT , CSV, DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "data_path = \"../savefiles/\" \n",
    "xtrain = load(data_path*\"xtrain_toyclassification.jld\")[\"xtrain\"]\n",
    "ytrain = load(data_path*\"ytrain_toyclassification.jld\")[\"ytrain\"]\n",
    "\n",
    "xtest = load(data_path*\"xtest_toyclassification.jld\")[\"xtest\"]\n",
    "ytest = load(data_path*\"ytest_toyclassification.jld\")[\"ytest\"]\n",
    "\n",
    "Xu = load(data_path*\"Xu_toyclassification.jld\")[\"Xu\"];\n",
    "M = length(Xu);\n",
    "N = length(ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_initial_params = (\n",
    "    k=(var=positive(1.0), precision=positive(1.0)),\n",
    "    m=zeros(M),\n",
    "    A=positive_definite(Matrix{Float64}(I, M, M)),\n",
    ");\n",
    "flat_init_params, unflatten = ParameterHandling.flatten(raw_initial_params)\n",
    "unpack = ParameterHandling.value ∘ unflatten;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lik = BernoulliLikelihood()\n",
    "jitter = 1e-3  # added to aid numerical stability\n",
    "\n",
    "function build_SVGP(params::NamedTuple)\n",
    "    kernel = params.k.var * with_lengthscale(SqExponentialKernel(),params.k.precision)\n",
    "    f = LatentGP(GP(kernel), lik, jitter)\n",
    "    q = MvNormal(params.m, params.A)\n",
    "    fz = f(Xu).fx\n",
    "    return SparseVariationalApproximation(fz, q), f\n",
    "end\n",
    "\n",
    "function loss(params::NamedTuple;x=xtrain,y=ytrain)\n",
    "    svgp, f = build_SVGP(params)\n",
    "    fx = f(xtrain)\n",
    "    return -elbo(svgp, fx, ytrain)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success (objective increased between iterations)\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     3.066228e+01\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     L-BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 2.25e-09 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 8.68e-10 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 7.11e-15 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 2.32e-16 ≰ 0.0e+00\n",
       "    |g(x)|                 = 9.15e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   3  (vs limit Inf)\n",
       "    Iterations:    130\n",
       "    f(x) calls:    371\n",
       "    ∇f(x) calls:   371\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimize(\n",
    "    loss ∘ unpack,\n",
    "    θ -> only(Zygote.gradient(loss ∘ unpack, θ)),\n",
    "    flat_init_params,\n",
    "    LBFGS(),\n",
    "    inplace=false,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = unpack(opt.minimizer)\n",
    "\n",
    "svgp_opt, f_opt = build_SVGP(final_params)\n",
    "post_opt = posterior(svgp_opt)\n",
    "l_post_opt = LatentGP(post_opt, BernoulliLikelihood(), jitter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mean = mean(post_opt(xtest))\n",
    "p_predict = normcdf.(predict_mean)\n",
    "\n",
    "predict_bin = zeros(length(p_predict))\n",
    "for i=1:length(p_predict)\n",
    "    p_predict[i] > 0.5 ? predict_bin[i] = 1.0 : predict_bin[i] = 0.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error_rate (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count number of errors (for classification)\n",
    "function num_error(ytrue, y)\n",
    "    return sum(abs.(y - ytrue))\n",
    "end\n",
    "\n",
    "function error_rate(ytrue, y)\n",
    "    return num_error(ytrue,y) / length(ytrue)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error:34.0\n",
      "Error rate: 0.085\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of error:\", num_error(ytest, predict_bin))\n",
    "println(\"Error rate: \", error_rate(ytest, predict_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banana dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "banana_data = CSV.read(\"../data/banana/banana.csv\", DataFrame);\n",
    "x_banana_data = [[banana_data[i,1], banana_data[i,2]] for i=1:size(banana_data,1)];\n",
    "banana_label = banana_data[:,end] |> (x) -> float(replace(x, -1 => 0));\n",
    "\n",
    "Ntrain = 4000;\n",
    "xtrain_banana, ytrain_banana = x_banana_data[1:Ntrain], banana_label[1:Ntrain];\n",
    "xtest_banana, ytest_banana = x_banana_data[Ntrain+1:end], banana_label[Ntrain + 1: end];\n",
    "\n",
    "batch_size = 200;\n",
    "\n",
    "#load inducing points \n",
    "Xu_banana = load(data_path*\"Xu_banana.jld\")[\"Xu\"]\n",
    "M_banana = length(Xu_banana)\n",
    "input_dim = length(Xu_banana[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct SVGPModel\n",
    "    k  :: Vector{Float64}     # kernel parameters\n",
    "    m_u  :: Vector{Float64}   # variational mean\n",
    "    A  :: Matrix{Float64}     # square-root of variational covariance\n",
    "end\n",
    "\n",
    "Flux.@functor SVGPModel (k, m_u, A);\n",
    "\n",
    "function make_kernel_banana(k_params)\n",
    "    variance = StatsFuns.softplus(k_params[1])\n",
    "    lengthscale = StatsFuns.softplus.(k_params[2:end])\n",
    "    return variance * with_lengthscale(SqExponentialKernel(), lengthscale)\n",
    "end\n",
    "\n",
    "jitter = 1e-5;\n",
    "function prior_banana(m::SVGPModel)\n",
    "    kernel = make_kernel_banana(m.k)\n",
    "    return LatentGP(GP(kernel),BernoulliLikelihood(),jitter)\n",
    "end\n",
    "\n",
    "function make_approx_banana(m::SVGPModel, prior)\n",
    "    # Efficiently constructs S as A*Aᵀ\n",
    "    S = PDMat(Cholesky(LowerTriangular(m.A)))\n",
    "    q = MvNormal(m.m_u, S)\n",
    "    fz = prior(Xu_banana).fx\n",
    "    return SparseVariationalApproximation(fz, q)\n",
    "end;\n",
    "\n",
    "function model_posterior_banana(m::SVGPModel)\n",
    "    svgp = make_approx_banana(m, prior_banana(m))\n",
    "    return posterior(svgp)\n",
    "end;\n",
    "\n",
    "function (m::SVGPModel)(x)\n",
    "    post = model_posterior_banana(m)\n",
    "    return post(x)\n",
    "end;\n",
    "\n",
    "function loss(m::SVGPModel, x, y; num_data=length(ytrain_banana))\n",
    "    f = prior_banana(m)\n",
    "    fx = f(x)\n",
    "    svgp = make_approx_banana(m, f)\n",
    "    return -elbo(svgp, fx, y; num_data)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_variance = 1\n",
    "init_lengthscale = ones(input_dim)\n",
    "k_banana_init = [StatsFuns.invsoftplus(init_variance), StatsFuns.invsoftplus.(init_lengthscale)] |> (x) -> vcat(x...);\n",
    "m_banana_init = zeros(M_banana)\n",
    "A_banana_init = Matrix{Float64}(I, M_banana, M_banana)\n",
    "\n",
    "model_banana = SVGPModel(k_banana_init, m_banana_init, A_banana_init);\n",
    "opt = Flux.AdaMax()  # Define the optimiser\n",
    "params_banana = Flux.params(model_banana);  # Extract the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = Flux.DataLoader((xtrain_banana, ytrain_banana);batchsize=batch_size)\n",
    "Flux.train!(\n",
    "    (x, y) -> loss(model_banana, x, y; num_data=Ntrain),\n",
    "    Flux.params(model_banana),\n",
    "    ncycle(data_loader,1000), #1000 epochs \n",
    "    opt,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_banana = model_posterior_banana(model_banana)\n",
    "my_predict_banana = post_banana(xtest_banana);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_banana = mean(my_predict_banana)\n",
    "p_predict_banana = normcdf.(predict_banana)\n",
    "\n",
    "predict_banana_bin = zeros(length(p_predict_banana))\n",
    "for i=1:length(p_predict_banana)\n",
    "    p_predict_banana[i] > 0.5 ? predict_banana_bin[i] = 1.0 : predict_banana_bin[i] = 0.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error:121.0\n",
      "Error rate: 0.09307692307692307\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of error:\", num_error(ytest_banana, predict_banana_bin))\n",
    "println(\"Error rate: \", error_rate(ytest_banana, predict_banana_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"../savefiles/VSGP_posterior_banana.jld\",\"posterior\",post_banana)\n",
    "save(\"../savefiles/VSGP_model_banana.jld\",\"model\",model_banana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot([opt_banana.trace[i].value for i=1:length(opt_banana.trace)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
