{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/GaussianProcessNode`\n"
     ]
    }
   ],
   "source": [
    "using Pkg \n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise \n",
    "using StableRNGs, GraphPPL,ReactiveMP, RxInfer, Random, Distributions, LinearAlgebra, Plots\n",
    "using Flux, Zygote, ForwardDiff\n",
    "using SpecialFunctions, Optim,ExponentialAction\n",
    "using BenchmarkTools\n",
    "import KernelFunctions: SqExponentialKernel, Matern52Kernel, with_lengthscale, Kernel, kernelmatrix  \n",
    "import ReactiveMP: GaussHermiteCubature, approximate_meancov, InverseWishartMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_data (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_data(n,f_gp, σ_x, σ_y)\n",
    "    obs = []\n",
    "    for i=1:n\n",
    "        x_t = σ_x * randn() \n",
    "        temp = x_t * exp(f_gp[i])    \n",
    "        push!(obs, temp)\n",
    "    end\n",
    "    return obs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1)\n",
    "\n",
    "σ_c = 2pi/30 # true frequency\n",
    "σ_y = 0.001 ## we assume low noise for now\n",
    "\n",
    "N = 400; #data length\n",
    "n = 400; #length of the axis of gp \n",
    "\n",
    "## GP information\n",
    "meanf = (x) -> 0.0;\n",
    "kernel(θ) = with_lengthscale(Matern52Kernel(),θ)\n",
    "tmin,tmax = 0., 4.0\n",
    "time_range = collect(range(tmin, tmax; length=n));\n",
    "\n",
    "θ_gp = 1. #true θ_gp\n",
    "\n",
    "Cov_mat = kernelmatrix(kernel(θ_gp),time_range,time_range) + 1e-8*I;\n",
    "gp = MvNormal(meanf.(time_range), Cov_mat)\n",
    "f_test = rand(gp)\n",
    "\n",
    "#Observe data \n",
    "pos = sort(randperm(n)[1:N]); # position where we observe data\n",
    "time_train = time_range[pos]\n",
    "f_train = f_test[pos];\n",
    "slicedim(dim) = (a) -> map(e -> e[dim], a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data \n",
    "y_data = generate_data(N,f_train,σ_c,σ_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponentiate function \n",
    "struct MyExponential end\n",
    "\n",
    "@node MyExponential Deterministic [ y, x ]   ## x: input,  y: output\n",
    "\n",
    "\n",
    "@rule MyExponential(:y, Marginalisation) (m_x::UnivariateGaussianDistributionsFamily,) = begin \n",
    "    return LogNormal(mean(m_x), var(m_x))\n",
    "end\n",
    "\n",
    "@rule MyExponential(:x, Marginalisation) (m_y::ContinuousUnivariateLogPdf, m_x::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    dist = m_x    \n",
    "    m_in, var_in = mean_var(m_x)\n",
    "    l_pdf = (x) -> logpdf(m_y,exp.(x)) \n",
    "    pdf = x -> exp(l_pdf(x)-logpdf(dist,x)+1e-7)\n",
    "    m,v = approximate_meancov(ghcubature(131),pdf,dist)\n",
    "\n",
    "    if isnan(v) || isnan(m)\n",
    "        log_pdf = x -> l_pdf(x) + logpdf(dist,x[1])  + 1e-7\n",
    "        res = optimize(x -> -log_pdf(x), [m_in])\n",
    "        mproxy = res.minimizer[1]\n",
    "        dx  = x -> ForwardDiff.derivative(y -> -log_pdf(y),x)\n",
    "        ddx = x -> ForwardDiff.derivative(dx, x)\n",
    "\n",
    "        vproxy = cholinv(ddx(mproxy+tiny))\n",
    "        m_ = mproxy \n",
    "        v_ = vproxy + 1e-6\n",
    "\n",
    "        ksi = m_/v_ - m_in/var_in\n",
    "        precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "        if isnan(ksi) || isnan(precision)\n",
    "            samples = rand(dist,2000)\n",
    "            weights = exp.(l_pdf.(samples)) / sum(exp.(l_pdf.(samples)) )\n",
    "            if any(isnan.(weights)) \n",
    "                m_ = sum(samples)/2000\n",
    "                v_ = sum((samples .- m_).^2) /2000 \n",
    "            else\n",
    "                m_ = sum(weights .* samples)\n",
    "                v_ = sum(weights .* (samples .- m_).^2)    \n",
    "            end\n",
    "            ksi = m_/v_ - m_in/var_in\n",
    "            precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "            \n",
    "            return NormalWeightedMeanPrecision(ksi,precision)\n",
    "        else\n",
    "            return  NormalWeightedMeanPrecision(ksi,precision)\n",
    "        end\n",
    "    else\n",
    "        return  NormalMeanVariance(m,v)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Gaussian GaussHermiteCubature to approximate \n",
    "#Temporarily put here \n",
    "function ReactiveMP.prod(::ProdAnalytical, left::UnivariateGaussianDistributionsFamily, right::ContinuousUnivariateLogPdf) \n",
    "    #we use Gaussian Hermite Cubature here, not ProdAnalytical \n",
    "#     meta = GaussHermiteCubature(121)\n",
    "#     m,v = ReactiveMP.approximate_meancov(meta, z -> exp(right.logpdf(z)), mean(left), var(left))\n",
    "#     return NormalMeanVariance(m,v)\n",
    "    sample_left = rand(left,500)\n",
    "    f_samples_evaluated = [exp(logpdf(right,sample_left[i])) for i=1:length(sample_left)]\n",
    "    p_samples_evaluated = [pdf(left,sample_left[i]) for i=1:length(sample_left)]\n",
    "    m = sum(sample_left .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    @show v = sum((sample_left .- m).^2 .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    if isnan(v)\n",
    "        k = sample_left .* p_samples_evaluated .* f_samples_evaluated\n",
    "        for i =1:length(k) \n",
    "            if isnan(k[i])\n",
    "                sample_left[i] = 1e-2\n",
    "            end\n",
    "        end\n",
    "        f_samples_evaluated = [exp(logpdf(right,sample_left[i])) for i=1:length(sample_left)]\n",
    "        p_samples_evaluated = [pdf(left,sample_left[i]) for i=1:length(sample_left)]\n",
    "        m = sum(sample_left .* p_samples_evaluated .* f_samples_evaluated)\n",
    "        v = sum((sample_left .- m).^2 .* p_samples_evaluated .* f_samples_evaluated) + 1e-6\n",
    "    end\n",
    "    @show m,v\n",
    "    return NormalMeanVariance(m,v)\n",
    "end\n",
    "\n",
    "function ReactiveMP.prod(::ProdAnalytical, left::ContinuousUnivariateLogPdf, right::UnivariateGaussianDistributionsFamily)\n",
    "#     meta = GaussHermiteCubature(121)\n",
    "#     m,v = ReactiveMP.approximate_meancov(meta, z -> exp(left.logpdf(z)), mean(right), var(right))\n",
    "    sample_right = rand(right,500)\n",
    "    f_samples_evaluated = [exp(logpdf(left,sample_right[i])) for i=1:length(sample_right)]\n",
    "    p_samples_evaluated = [pdf(right,sample_right[i]) for i=1:length(sample_right)]\n",
    "    m = sum(sample_right .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    v = sum((sample_right .- m).^2 .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    if isnan(v)\n",
    "        k = sample_right .* p_samples_evaluated .* f_samples_evaluated\n",
    "        for i =1:length(k) \n",
    "            if isnan(k[i])\n",
    "                sample_right[i] = 1e-2\n",
    "            end\n",
    "        end\n",
    "        f_samples_evaluated = [exp(logpdf(left,sample_right[i])) for i=1:length(sample_right)]\n",
    "        p_samples_evaluated = [pdf(right,sample_right[i]) for i=1:length(sample_right)]\n",
    "        m = sum(sample_right .* p_samples_evaluated .* f_samples_evaluated)\n",
    "        v = sum((sample_right .- m).^2 .* p_samples_evaluated .* f_samples_evaluated) +1e-6\n",
    "    end\n",
    "    @show m,v\n",
    "    return NormalMeanVariance(m,v)\n",
    "end\n",
    "\n",
    "function ReactiveMP.prod(::ProdAnalytical, left::LogNormal, right::ContinuousUnivariateLogPdf) \n",
    "    sample_left = rand(left,500)\n",
    "    f_samples_evaluated = [exp(logpdf(right,sample_left[i])) for i=1:length(sample_left)]\n",
    "    p_samples_evaluated = [pdf(left,sample_left[i]) for i=1:length(sample_left)]\n",
    "    m = sum(sample_left .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    v = sum((sample_left .- m).^2 .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    if isnan(v)\n",
    "        k = sample_left .* p_samples_evaluated .* f_samples_evaluated\n",
    "        for i =1:length(k) \n",
    "            if isnan(k[i])\n",
    "                sample_left[i] = 1e-2\n",
    "            end\n",
    "        end\n",
    "        f_samples_evaluated = [exp(logpdf(right,sample_left[i])) for i=1:length(sample_left)]\n",
    "        p_samples_evaluated = [pdf(left,sample_left[i]) for i=1:length(sample_left)]\n",
    "        m = sum(sample_left .* p_samples_evaluated .* f_samples_evaluated)\n",
    "        v = sum((sample_left .- m).^2 .* p_samples_evaluated .* f_samples_evaluated)\n",
    "    end\n",
    "    return LogNormal(m,v)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rule typeof(*)(:A, Marginalisation) (m_out::UnivariateGaussianDistributionsFamily, m_A::UnivariateGaussianDistributionsFamily, m_in::LogNormal, meta::TinyCorrection) = begin \n",
    "    #mean_in and var_in come from m_A\n",
    "    backward_A = @call_rule typeof(*)(:A, Marginalisation) (m_out = m_out, m_in = m_in, meta=meta)\n",
    "    mean_in, var_in = mean_var(m_A)\n",
    "    dist = m_A \n",
    "    pdf = x -> exp(backward_A(x)-logpdf(dist,x)+1e-7)\n",
    "    m,v = approximate_meancov(ghcubature(121),pdf,dist)\n",
    "\n",
    "    if isnan(v) || isnan(m)\n",
    "        log_pdf = x -> backward_A(x) + logpdf(dist,x[1])  + 1e-7\n",
    "        res = optimize(x -> -log_pdf(x), [mean_in])\n",
    "        mproxy = res.minimizer[1]\n",
    "        dx  = x -> ForwardDiff.derivative(y -> -log_pdf(y),x)\n",
    "        ddx = x -> ForwardDiff.derivative(dx, x)\n",
    "\n",
    "        vproxy = cholinv(ddx(mproxy+tiny))\n",
    "        m_ = mproxy \n",
    "        v_ = vproxy + 1e-6\n",
    "\n",
    "        ksi = m_/v_ - mean_in/var_in\n",
    "        precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "        if isnan(ksi) || isnan(precision)\n",
    "            samples = rand(dist,3000)\n",
    "            weights = exp.(backward_A.(samples)) / sum(exp.(backward_A.(samples)) )\n",
    "            if any(isnan.(weights)) \n",
    "                m_ = sum(samples)/3000\n",
    "                v_ = sum((samples .- m_).^2) /3000\n",
    "            else\n",
    "                m_ = sum(weights .* samples)\n",
    "                v_ = sum(weights .* (samples .- m_).^2)    \n",
    "            end\n",
    "            ksi = m_/v_ - mean_in/var_in\n",
    "            precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "            \n",
    "            return NormalWeightedMeanPrecision(ksi,precision)\n",
    "        else\n",
    "            return  NormalWeightedMeanPrecision(ksi,precision)\n",
    "        end\n",
    "    else\n",
    "        return  NormalMeanVariance(m,v)\n",
    "    end\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:A, Marginalisation) (m_out::PointMass, m_A::UnivariateGaussianDistributionsFamily, m_in::LogNormal, meta::TinyCorrection) = begin \n",
    "    backward_A = (x) -> -log(abs(x)) + logpdf(m_in,mean(m_out)/x)\n",
    "    mean_in, var_in = mean_var(m_A)\n",
    "    dist = m_A \n",
    "    pdf = x -> exp(backward_A(x)-logpdf(dist,x)+1e-7)\n",
    "    m,v = approximate_meancov(ghcubature(121),pdf,dist)\n",
    "\n",
    "    if isnan(v) || isnan(m)\n",
    "        log_pdf = x -> backward_A(x[1]) + logpdf(dist,x[1])  + 1e-7\n",
    "        res = optimize(x -> -log_pdf(x), [mean_in])\n",
    "        mproxy = res.minimizer[1]\n",
    "        dx  = x -> ForwardDiff.derivative(y -> -log_pdf(y),x)\n",
    "        ddx = x -> ForwardDiff.derivative(dx, x)\n",
    "\n",
    "        vproxy = cholinv(ddx(mproxy+tiny))\n",
    "        m_ = mproxy \n",
    "        v_ = vproxy + 1e-6\n",
    "\n",
    "        ksi = m_/v_ - mean_in/var_in\n",
    "        precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "        if isnan(ksi) || isnan(precision)\n",
    "            samples = rand(dist,3000)\n",
    "            weights = exp.(backward_A.(samples)) / sum(exp.(backward_A.(samples)) )\n",
    "            if any(isnan.(weights)) \n",
    "                m_ = sum(samples)/3000\n",
    "                v_ = sum((samples .- m_).^2) /3000\n",
    "            else\n",
    "                m_ = sum(weights .* samples)\n",
    "                v_ = sum(weights .* (samples .- m_).^2)    \n",
    "            end\n",
    "            ksi = m_/v_ - mean_in/var_in\n",
    "            precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "            \n",
    "            return NormalWeightedMeanPrecision(ksi,precision)\n",
    "        else\n",
    "            return  NormalWeightedMeanPrecision(ksi,precision)\n",
    "        end\n",
    "    else\n",
    "        return  NormalMeanVariance(m,v)\n",
    "    end\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:A, Marginalisation) (m_out::PointMass, m_in::UnivariateGaussianDistributionsFamily, meta::TinyCorrection) = begin \n",
    "    backward_A = (x) -> -log(abs(x)) + logpdf(m_in,mean(m_out)/x)\n",
    "    return ContinuousUnivariateLogPdf(backward_A)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node ComputeMatP\n",
    "struct ComputeMatP end\n",
    "\n",
    "@node ComputeMatP Deterministic [ P, λ ]   ## λ: input (already in log-scale),  P: output\n",
    "\n",
    "@rule ComputeMatP(:P, Marginalisation) (m_λ::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    λ = mean(m_λ)\n",
    "    P  = compute_P∞(λ)\n",
    "    P_vec = vec(P)\n",
    "    d = length(P)\n",
    "    return MvNormalMeanCovariance(P_vec, 0.1*diageye(d))\n",
    "end\n",
    "\n",
    "@rule ComputeMatP(:λ, Marginalisation) (m_P::MultivariateGaussianDistributionsFamily, ) = begin \n",
    "    Pvec = (x) -> vec(compute_P∞(x))\n",
    "    backwardpass = (x) -> logpdf(m_P, Pvec(x))\n",
    "    return ContinuousUnivariateLogPdf(backwardpass)\n",
    "end\n",
    "\n",
    "@rule ComputeMatP(:λ, Marginalisation) (m_P::MultivariateGaussianDistributionsFamily, m_λ::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    Pvec = (x) -> vec(compute_P∞(x))\n",
    "    backward_λ = (x) -> logpdf(m_P, Pvec(x))\n",
    "    mean_in, var_in = mean_var(m_λ)\n",
    "    dist = m_λ\n",
    "\n",
    "    samples = rand(dist,3000)\n",
    "    weights = exp.(backward_λ.(samples)) / sum(exp.(backward_λ.(samples)) )\n",
    "    if any(isnan.(weights)) \n",
    "        m_ = sum(samples)/3000\n",
    "        v_ = sum((samples .- m_).^2) /3000\n",
    "    else\n",
    "        m_ = sum(weights .* samples)\n",
    "        v_ = sum(weights .* (samples .- m_).^2)    \n",
    "    end\n",
    "    ksi = m_/v_ - mean_in/var_in\n",
    "    precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "    return NormalWeightedMeanPrecision(ksi,precision)\n",
    "end\n",
    "    \n",
    "#node ComputeMatA\n",
    "struct ComputeMatA end \n",
    "\n",
    "@node ComputeMatA Deterministic [ A, λ ]   ## λ: input,  A: output\n",
    "\n",
    "@rule ComputeMatA(:A, Marginalisation) (m_λ::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    λ = mean(m_λ)\n",
    "    \n",
    "    A = compute_A(λ)\n",
    "    A_vec = vec(A)\n",
    "    d = length(A)\n",
    "    return MvNormalMeanCovariance(A_vec, 0.1*diageye(d))\n",
    "end\n",
    "\n",
    "@rule ComputeMatA(:λ, Marginalisation) (m_A::MultivariateGaussianDistributionsFamily, ) = begin \n",
    "    Avec = (x) -> vec(compute_A(x))\n",
    "    backwardpass = (x) -> logpdf(m_A, Avec(x))\n",
    "    return ContinuousUnivariateLogPdf(backwardpass)\n",
    "end\n",
    "        \n",
    "        \n",
    "@rule ComputeMatA(:λ, Marginalisation) (m_A::MultivariateGaussianDistributionsFamily, m_λ::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    Avec = (x) -> vec(compute_A(x))\n",
    "    backward_λ = (x) -> logpdf(m_A, Avec(x))\n",
    "    mean_in, var_in = mean_var(m_λ)\n",
    "    dist = m_λ\n",
    "            \n",
    "    samples = rand(dist,3000)\n",
    "    weights = exp.(backward_λ.(samples)) / sum(exp.(backward_λ.(samples)) )\n",
    "    if any(isnan.(weights)) \n",
    "        m_ = sum(samples)/3000\n",
    "        v_ = sum((samples .- m_).^2) /3000\n",
    "    else\n",
    "        m_ = sum(weights .* samples)\n",
    "        v_ = sum(weights .* (samples .- m_).^2)    \n",
    "    end\n",
    "    ksi = m_/v_ - mean_in/var_in\n",
    "    precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "            \n",
    "    return NormalWeightedMeanPrecision(ksi,precision)\n",
    "end\n",
    "\n",
    "#node ComputeMatQ\n",
    "struct ComputeMatQ end \n",
    "\n",
    "@node ComputeMatQ Deterministic [ Q, λ ]   ## λ: input,  Q: output\n",
    "\n",
    "@rule ComputeMatQ(:Q, Marginalisation) (m_λ::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    λ  = mean(m_λ)\n",
    "    Q  = compute_Q(λ)\n",
    "    Q_vec = vec(Q)\n",
    "    d = length(Q)\n",
    "    return MvNormalMeanCovariance(Q_vec, 0.1*diageye(d))\n",
    "end\n",
    "\n",
    "@rule ComputeMatQ(:λ, Marginalisation) (m_Q::MultivariateGaussianDistributionsFamily, ) = begin \n",
    "    Qvec = (x) -> vec(compute_Q(x))\n",
    "    backwardpass = (x) -> logpdf(m_Q, Qvec(x))\n",
    "    return ContinuousUnivariateLogPdf(backwardpass)\n",
    "end\n",
    "            \n",
    "            \n",
    "@rule ComputeMatQ(:λ, Marginalisation) (m_Q::MultivariateGaussianDistributionsFamily, m_λ::UnivariateGaussianDistributionsFamily, ) = begin \n",
    "    Qvec = (x) -> vec(compute_Q(x))\n",
    "    backward_λ = (x) -> logpdf(m_Q, Qvec(x))\n",
    "    mean_in, var_in = mean_var(m_λ)\n",
    "    dist = m_λ\n",
    "                \n",
    "    samples = rand(dist,3000)\n",
    "    weights = exp.(backward_λ.(samples)) / sum(exp.(backward_λ.(samples)) )\n",
    "    if any(isnan.(weights)) \n",
    "        m_ = sum(samples)/3000\n",
    "        v_ = sum((samples .- m_).^2) /3000\n",
    "    else\n",
    "        m_ = sum(weights .* samples)\n",
    "        v_ = sum(weights .* (samples .- m_).^2)    \n",
    "    end\n",
    "    ksi = m_/v_ - mean_in/var_in\n",
    "    precision = clamp(1/v_ - 1/var_in, tiny,huge)\n",
    "\n",
    "    return  NormalWeightedMeanPrecision(ksi,precision)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ReShape end \n",
    "@node ReShape Deterministic [matrix, vector]  # this node transforms a vector to matrix and vice versa\n",
    "\n",
    "@rule ReShape(:matrix, Marginalisation) (m_vector::MultivariateGaussianDistributionsFamily, ) = begin \n",
    "    mean_vector = mean(m_vector)\n",
    "    d = length(mean_vector)\n",
    "    n = Int(sqrt(d))\n",
    "    M = reshape(mean_vector,n,n)\n",
    "    return Tuple([M])\n",
    "end\n",
    "\n",
    "@rule ReShape(:vector, Marginalisation) (m_matrix::Tuple, ) = begin \n",
    "    matrix = m_matrix[1]\n",
    "    vector = vec(matrix)\n",
    "    d = length(matrix)\n",
    "    return MvNormalMeanCovariance(vector, 0.1*diageye(d))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rule MvNormalMeanCovariance(:out, Marginalisation) (m_μ::MultivariateGaussianDistributionsFamily, m_Σ::Tuple, ) = begin \n",
    "    μ_mean, μ_cov = mean_cov(m_μ)\n",
    "    Σ = m_Σ[1]\n",
    "    return MvNormalMeanCovariance(μ_mean, μ_cov + Σ)\n",
    "#     return MvNormalMeanCovariance(μ_mean, Σ) #VMP rule\n",
    "end\n",
    "\n",
    "@rule MvNormalMeanCovariance(:out, Marginalisation) (m_μ::PointMass, m_Σ::Tuple, ) = begin \n",
    "    Σ = m_Σ[1]\n",
    "    return MvNormalMeanCovariance(mean(m_μ), Σ)\n",
    "end\n",
    "\n",
    "@rule MvNormalMeanCovariance(:Σ, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, m_μ::MultivariateGaussianDistributionsFamily, ) = begin\n",
    "    m_mean, v_mean = mean_cov(m_μ)\n",
    "    m_out, v_out = mean_cov(m_out)\n",
    "    S  = v_mean + v_out + (m_mean - m_out) * (m_mean - m_out)'\n",
    "    return Tuple([S])\n",
    "end\n",
    "\n",
    "@rule MvNormalMeanCovariance(:out, Marginalisation) (m_Σ::Tuple, q_μ::PointMass, ) = begin \n",
    "    Σ = m_Σ[1]\n",
    "    return MvNormalMeanCovariance(mean(q_μ), Σ)\n",
    "end\n",
    "\n",
    "@rule MvNormalMeanCovariance(:Σ, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, q_μ::PointMass, ) = begin \n",
    "    m_mean = mean(q_μ)\n",
    "    m_out, v_out = mean_cov(m_out)\n",
    "    S  = v_out + (m_mean - m_out) * (m_mean - m_out)'\n",
    "    return Tuple([S])\n",
    "end\n",
    "\n",
    "@rule MvNormalMeanCovariance(:μ, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, m_Σ::Tuple, ) = begin \n",
    "    m_out_mean, m_out_cov = mean_cov(m_out)\n",
    "    Σ = m_Σ[1]\n",
    "    return MvNormalMeanCovariance(m_out_mean, m_out_cov + Σ)\n",
    "#     return MvNormalMeanCovariance(m_out_mean, Σ) # VMP rule \n",
    "end\n",
    "\n",
    "### additional rules for multiplication\n",
    "@rule typeof(*)(:A, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, m_in::MultivariateGaussianDistributionsFamily, meta::TinyCorrection) = begin\n",
    "    ybar, invW = mean_cov(m_out)\n",
    "    xbar, Σx = mean_cov(m_in)\n",
    "\n",
    "    A = ybar * xbar' * inv(Σx + xbar*xbar')\n",
    "    return Tuple([A])\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:in, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, m_A::Tuple, meta::TinyCorrection) = begin\n",
    "    A = m_A[1]\n",
    "    μ_out, Σ_out = mean_cov(m_out)\n",
    "\n",
    "    z = ReactiveMP.fastcholesky(Σ_out)\n",
    "    tmp = A' / z\n",
    "    W = ReactiveMP.correction!(meta, tmp * A)\n",
    "\n",
    "    return MvNormalWeightedMeanPrecision(tmp * μ_out, W)\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:A, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, m_in::Tuple, meta::TinyCorrection) = begin\n",
    "    A = m_in[1]\n",
    "    μ_out, Σ_out = mean_cov(m_out)\n",
    "\n",
    "    z = ReactiveMP.fastcholesky(Σ_out)\n",
    "    tmp = A' / z\n",
    "    W = ReactiveMP.correction!(meta, tmp * A)\n",
    "\n",
    "    return MvNormalWeightedMeanPrecision(tmp * μ_out, W)\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:in, Marginalisation) (m_out::MultivariateGaussianDistributionsFamily, m_A::MultivariateGaussianDistributionsFamily, meta::TinyCorrection) = begin\n",
    "    return @call_rule typeof(*)(:A, Marginalisation) (m_out = m_out, m_in=m_A, meta=meta)\n",
    "end\n",
    "\n",
    "@rule typeof(*)(:out, Marginalisation) (m_A::Tuple, m_in::MultivariateGaussianDistributionsFamily, meta::TinyCorrection) = begin\n",
    "    A = m_A[1]\n",
    "    μ_in, Σ_in = mean_cov(m_in)\n",
    "    return MvNormalMeanCovariance(A * μ_in, A * Σ_in * A')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function gpPAD_params(n, l_λ_init,H)\n",
    "    y = datavar(Float64,n)\n",
    "    f = randomvar(n)\n",
    "    z = randomvar(n)\n",
    "    g = randomvar(n)\n",
    "    c = randomvar(n)\n",
    "    w = randomvar(n)\n",
    "    B = randomvar(n)\n",
    "    A = randomvar(n)\n",
    "    Avec = randomvar(n)\n",
    "    Qvec = randomvar(n)\n",
    "    Q = randomvar(n)\n",
    "    \n",
    "    l_λ ~ NormalMeanVariance(l_λ_init, 10.)\n",
    "    P∞_vec ~ ComputeMatP(l_λ) where {pipeline = RequireMessage(λ = NormalMeanVariance(l_λ_init, 10.))}\n",
    "    P∞ ~ ReShape(P∞_vec)\n",
    "    f_0 ~ MvNormalMeanCovariance(zeros(length(H)), P∞)\n",
    "    \n",
    "    f_prev = f_0\n",
    "    for i=1:n \n",
    "        Avec[i] ~ ComputeMatA(l_λ) where {pipeline = RequireMessage(λ = NormalMeanVariance(l_λ_init, 10.))}\n",
    "        A[i] ~ ReShape(Avec[i])\n",
    "        Qvec[i] ~ ComputeMatQ(l_λ) where {pipeline = RequireMessage(λ = NormalMeanVariance(l_λ_init, 10.))}\n",
    "        Q[i] ~ ReShape(Qvec[i])\n",
    "        B[i] ~ (*)(A[i],f_prev) where {meta = TinyCorrection()}\n",
    "        f[i] ~ MvNormalMeanCovariance(B[i], Q[i])\n",
    "        z[i] ~ NormalMeanVariance(dot(H , f[i]), 0.01)\n",
    "\n",
    "        g[i] ~ MyExponential(z[i]) where {pipeline = RequireMessage(x = NormalMeanPrecision(0., 0.01))}\n",
    "        c[i] ~ NormalMeanPrecision(0.0,1/(σ_c)^2) \n",
    "        w[i] ~ (*)(c[i],g[i]) where {meta = TinyCorrection()}\n",
    "        y[i] ~ NormalMeanPrecision(w[i], 1/σ_y^2)\n",
    "        f_prev = f[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_Q (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_F(logλ)\n",
    "    F = [0. 1. 0.; 0. 0. 1.;-exp(logλ[])^3 -3exp(logλ[])^2 -3exp(logλ[])]\n",
    "    return F \n",
    "end\n",
    "\n",
    "function compute_A(logλ)\n",
    "    F = [0. 1. 0.; 0. 0. 1.;-exp(logλ[])^3 -3exp(logλ[])^2 -3exp(logλ[])]\n",
    "    Imat = diageye(3)\n",
    "    A = expv(Δt,F,Imat)\n",
    "    return A \n",
    "end\n",
    "\n",
    "function compute_P∞(logλ)\n",
    "    F = [0. 1. 0.; 0. 0. 1.;-exp(logλ[])^3 -3exp(logλ[])^2 -3exp(logλ[])]\n",
    "    Imat = diageye(3)\n",
    "    Qc = 16/3 * exp(logλ[])^5\n",
    "    L = [0., 0., 1.];\n",
    "    vec_P = inv(kron(Imat,F) + kron(F,Imat)+ 1e-8*diageye(9)) * vec(-L * Qc * L')\n",
    "    P∞ = reshape(vec_P,3,3)\n",
    "    return P∞\n",
    "end\n",
    "\n",
    "function compute_Q(logλ)\n",
    "    F = [0. 1. 0.; 0. 0. 1.;-exp(logλ[])^3 -3exp(logλ[])^2 -3exp.(logλ[])]\n",
    "    Imat = diageye(3)\n",
    "    A = expv(Δt,F,Imat)\n",
    "    Qc = 16/3 * exp(logλ[])^5\n",
    "    L = [0., 0., 1.];\n",
    "    vec_P = inv(kron(Imat,F) + kron(F,Imat)+ 1e-8*diageye(9)) * vec(-L * Qc * L')\n",
    "    P∞ = reshape(vec_P,3,3)\n",
    "    Q = P∞ - A*P∞*A'\n",
    "    return Q \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δt = time_range[2] - time_range[1]; # time difference\n",
    "\n",
    "l_init = 1.;\n",
    "λ_init = sqrt(5)/l_init;\n",
    "logλ_init = log(λ_init);\n",
    "\n",
    "L = [0., 0., 1.];\n",
    "H = [1., 0., 0.];\n",
    "F_init = [0. 1. 0.; 0. 0. 1.;-λ_init^3 -3λ_init^2 -3λ_init]\n",
    "Qc = 16/3 * λ_init^5;\n",
    "\n",
    "Imat = diageye(3) ; \n",
    "vec_P = inv(kron(Imat,F_init) + kron(F_init,Imat)) * vec(-L * Qc * L'); \n",
    "P∞_init = reshape(vec_P,3,3);\n",
    "A_init = exp(F_init * Δt)\n",
    "Q = P∞_init - A_init * P∞_init * A_init';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Distributions.logkernel(d::InverseWishartMessage, X::AbstractMatrix)\n",
    "    p = size(d, 1)\n",
    "    df = d.ν\n",
    "\n",
    "    Xcf = ReactiveMP.fastcholesky(X + 1e-6*ones(p,p))\n",
    "    # we use the fact: tr(Ψ * inv(X)) = tr(inv(X) * Ψ) = tr(X \\ Ψ)\n",
    "    Ψ = Matrix(d.S)\n",
    "    -0.5 * ((df + p + 1) * logdet(Xcf) + tr(Xcf \\ Ψ))\n",
    "end\n",
    "\n",
    "function Distributions.logkernel(d::InverseWishart, X::AbstractMatrix)\n",
    "    p = size(d, 1)\n",
    "    df = d.df\n",
    "\n",
    "    Xcf = ReactiveMP.fastcholesky(X + 1e-6*ones(p,p))\n",
    "    # we use the fact: tr(Ψ * inv(X)) = tr(inv(X) * Ψ) = tr(X \\ Ψ)\n",
    "    Ψ = Matrix(d.Ψ)\n",
    "    -0.5 * ((df + p + 1) * logdet(Xcf) + tr(Xcf \\ Ψ))\n",
    "end\n",
    "\n",
    "function Distributions.invwishart_logc0(df::Real, Ψ::AbstractMatrix)\n",
    "    h_df = -df / 2\n",
    "    p = size(Ψ, 1)\n",
    "    -h_df * (p * typeof(df)(log(2)) - logdet(Ψ)) - Distributions.logmvgamma(p, h_df)\n",
    "end\n",
    "\n",
    "Distributions._logpdf(d::InverseWishartMessage, X::AbstractMatrix{<:Real}) = Distributions.logkernel(d, X) + Distributions.invwishart_logc0(d.ν, d.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ReactiveMP.prod(::ProdAnalytical, left::Tuple{Matrix{Float64}}, right::Tuple{Matrix{Float64}})\n",
    "    left_matrix = left[1]\n",
    "    right_matrix = right[1]\n",
    "    return Tuple([left_matrix .* right_matrix])\n",
    "#     return right\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nits =1\n",
    "\n",
    "iresult = inference(\n",
    "    model = gpPAD_params(length(y_data),logλ_init,H),\n",
    "    iterations = nits, \n",
    "    data  = (y = y_data,),\n",
    "    initmessages = (l_λ = NormalMeanVariance(logλ_init, 10.),),\n",
    "    returnvars = (l_λ = KeepLast(),f = KeepLast(),),\n",
    "    options = (limit_stack_depth=500,),\n",
    "    # constraints = gp_PAD_constraints(),\n",
    "    showprogress = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: iresult not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: iresult not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[20]:1"
     ]
    }
   ],
   "source": [
    "log_λ =  mean(iresult.posteriors[:l_λ])\n",
    "λ = exp(log_λ)\n",
    "l = sqrt(5) / λ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp = mean.(iresult.posteriors[:f]) |> slicedim(1)\n",
    "vgp = var.(iresult.posteriors[:f]) |> slicedim(1)\n",
    "expvgp = exp.( mgp .+ vgp)\n",
    "expmgp = exp.(mgp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: expvgp not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: expvgp not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[22]:1"
     ]
    }
   ],
   "source": [
    "plot(time_range,expmgp,ribbon=sqrt.(expvgp),linewidth=3.0,label=\"modulator GP-estimated\",fillalpha=0.2,color=:green, size = (700, 400))\n",
    "plot!(time_range,exp.(f_test), label=\"modulator GP-true\",linewidth=3.0,color=:blue)\n",
    "plot!(time_range, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
